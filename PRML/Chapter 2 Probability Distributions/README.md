# PRML Chapter 2: Probability Distributions

## 2.1 Binary Variables

Consider a binary variable $x \in \{0,1\}$ which $x=1$ respresents the head of a coin and $x=0$ represents the tail of a coin. The probability of the head of a coin is given by $P(x=1) = \mu$ and the probability of the tail of a coin is given by $P(x=0) = 1-\mu$. The probability of the binary variable $x$ can be written as:

$$
P(x|\mu) = \mu^x(1-\mu)^{1-x}
$$

where $\mu \in [0,1]$ is the parameter of the distribution. This distribution is called the Bernoulli distribution. When $x=1$, the probability of the head of a coin is given by $\mu$ and when $x=0$, the probability of the tail of a coin is given by $1-\mu$. The expected value of the Bernoulli distribution is given by:

$$
E[x] = \sum_{x=0}^{1} xP(x) = 0 \times (1-\mu) + 1 \times \mu = \mu
$$

The variance of the Bernoulli distribution is given by:

$$
Var[x] = E[x^2] - E[x]^2 = \mu - \mu^2 = \mu(1-\mu)
$$

And suppose we have a data set $\mathcal{D} = \{x_1,x_2,\ldots,x_N\}$ where $x_i \in \{0,1\}$ and the data set is generated by a coin with probability $\mu$. The likelihood function of the data set $\mathcal{D}$ is given by:

$$
P(\mathcal{D}|\mu) = \prod_{n=1}^{N} P(x_n|\mu) = \prod_{n=1}^{N} \mu^{x_n}(1-\mu)^{1-x_n}
$$

The likelihood function means that the probability of the data set $\forall x \in \mathcal{D}$, the events $x$ all occur. And the log likelihood function is given by:

$$
\ln P(\mathcal{D}|\mu) = \sum_{n=1}^{N} \ln P(x_n|\mu) = \sum_{n=1}^{N} \{x_n \ln \mu + (1-x_n) \ln (1-\mu)\}
$$

Due to the $\mu$ can be seen as a constant, so the log likelihood function is only depends on the sum of the observed data points $x_n$. We want to maximize the likelihood function, so we can take derivative of the log likelihood function with respect to $\mu$ and set it to zero:

$$
\frac{d}{d\mu} \ln P(\mathcal{D}|\mu) = \sum_{n=1}^{N} \left( \frac{x_n}{\mu} - \frac{1-x_n}{1-\mu} \right) = 0
$$

And we can solve the above equation to get the maximum likelihood estimator of $\mu$:

$$
\mu_{ML} = \frac{1}{N} \sum_{n=1}^{N} x_n
$$

Which means if we want to estimate the probability of the head of a coin, we can just calculate the proportion of the head of a coin in the data set. Using the statistical average of the dataset to apporximate the probability of the head of a coin is a reasonable method. It can maximize the likelihood function of the data set. And then, have the minimized error of the estimation.

For $x$ is a binary value, if we count the number of the head of a coin in the dataset is $m$ and the length of dataset is $N$, the probability of the head of a coin is given by:

$$
\mu_{ML} = \frac{m}{N}
$$

If we observe 3 data points and the data set is $\mathcal{D} = \{1,1,1\}$, the probability of the head of a coin is given by:

$$
\mu_{ML} = \frac{3}{3} = 1
$$

$\mu_{ML}$ is used to do decision-making when we meet new data. So it is obviously impossible in real world that the coin is always the head. The phenomenon is called overfitting as we only want to maximize the likelihood function.

If we have $m$ numbers that $x=1$ occurs and the total number of the data set is $N$, since each event is independent, the probability of this is called the binomial distribution:

$$
P(m|N,\mu) = \binom{N}{m} \mu^m(1-\mu)^{N-m}
$$

where $\binom{N}{m}$ is the binomial coefficient. This is says that we will choose $m$ events that happen in total $N$ events and another $N-m$ events that do not happen. So there will be a binomial coefficient $\binom{N}{m}$ ways to choose $m$ events from $N$ events. The expected value of the binomial distribution is given by:

$$
E[m] = \sum_{m=0}^{N} mP(m) = N\mu
$$

The variance of the binomial distribution is given by:

$$
Var[m] = E[m^2] - E[m]^2 = N\mu(1-\mu)
$$

### 2.1.1 The Beta Distribution

We have discussed above that if we only maximize the likelihood function, we may meet over-fitting problem. And the key to Bayesian theory is using prior probility to estimate the posterior probability. Previously, we only consider $\mu$ is a constant, we can regard $\mu$ as a random variable and use the prior probability $p(\mu)$ to estimate the posterior probability.