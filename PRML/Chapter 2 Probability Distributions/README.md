# PRML Chapter 2: Probability Distributions


Likelihood and probability: The probability of the data is when we know the parameters of the model, which means in the case of parameters $\theta$, the events $A$ occur which can denoted as $P(A|\theta)$. The likelihood of the parameters is the probability of the parameters when we know the data, which means in the case of data $D$, the parameters $\theta$ occur which can denoted as $P(\theta|D)$. The likelihood function is given by $P(D|\theta)$. We need to do MLE because MLE can find the parameters that maximize the likelihood function, which means the probability of the events $A$ occur is maximized.

## 2.1 Binary Variables

Consider a binary variable $x \in \{0,1\}$ which $x=1$ respresents the head of a coin and $x=0$ represents the tail of a coin. The probability of the head of a coin is given by $P(x=1) = \mu$ and the probability of the tail of a coin is given by $P(x=0) = 1-\mu$. The probability of the binary variable $x$ can be written as:

$$
P(x|\mu) = \mu^x(1-\mu)^{1-x}
$$

where $\mu \in [0,1]$ is the parameter of the distribution. This distribution is called the Bernoulli distribution. When $x=1$, the probability of the head of a coin is given by $\mu$ and when $x=0$, the probability of the tail of a coin is given by $1-\mu$. The expected value of the Bernoulli distribution is given by:

$$
E[x] = \sum_{x=0}^{1} xP(x) = 0 \times (1-\mu) + 1 \times \mu = \mu
$$

The variance of the Bernoulli distribution is given by:

$$
Var[x] = E[x^2] - E[x]^2 = \mu - \mu^2 = \mu(1-\mu)
$$

And suppose we have a data set $\mathcal{D} = \{x_1,x_2,\ldots,x_N\}$ where $x_i \in \{0,1\}$ and the data set is generated by a coin with probability $\mu$. The likelihood function of the data set $\mathcal{D}$ is given by:

$$
P(\mathcal{D}|\mu) = \prod_{n=1}^{N} P(x_n|\mu) = \prod_{n=1}^{N} \mu^{x_n}(1-\mu)^{1-x_n}
$$

The likelihood function means that the probability of the data set $\forall x \in \mathcal{D}$, the events $x$ all occur. And the log likelihood function is given by:

$$
\ln P(\mathcal{D}|\mu) = \sum_{n=1}^{N} \ln P(x_n|\mu) = \sum_{n=1}^{N} \{x_n \ln \mu + (1-x_n) \ln (1-\mu)\}
$$

Due to the $\mu$ can be seen as a constant, so the log likelihood function is only depends on the sum of the observed data points $x_n$. We want to maximize the likelihood function, so we can take derivative of the log likelihood function with respect to $\mu$ and set it to zero:

$$
\frac{d}{d\mu} \ln P(\mathcal{D}|\mu) = \sum_{n=1}^{N} \left( \frac{x_n}{\mu} - \frac{1-x_n}{1-\mu} \right) = 0
$$

And we can solve the above equation to get the maximum likelihood estimator of $\mu$:

$$
\mu_{ML} = \frac{1}{N} \sum_{n=1}^{N} x_n
$$

Which means if we want to estimate the probability of the head of a coin, we can just calculate the proportion of the head of a coin in the data set. Using the statistical average of the dataset to apporximate the probability of the head of a coin is a reasonable method. It can maximize the likelihood function of the data set. And then, have the minimized error of the estimation.

For $x$ is a binary value, if we count the number of the head of a coin in the dataset is $m$ and the length of dataset is $N$, the probability of the head of a coin is given by:

$$
\mu_{ML} = \frac{m}{N}
$$

If we observe 3 data points and the data set is $\mathcal{D} = \{1,1,1\}$, the probability of the head of a coin is given by:

$$
\mu_{ML} = \frac{3}{3} = 1
$$

$\mu_{ML}$ is used to do decision-making when we meet new data. So it is obviously impossible in real world that the coin is always the head. The phenomenon is called overfitting as we only want to maximize the likelihood function.

If we have $m$ numbers that $x=1$ occurs and the total number of the data set is $N$, since each event is independent, the probability of this is called the binomial distribution:

$$
P(m|N,\mu) = \binom{N}{m} \mu^m(1-\mu)^{N-m}
$$

where $\binom{N}{m}$ is the binomial coefficient. This is says that we will choose $m$ events that happen in total $N$ events and another $N-m$ events that do not happen. So there will be a binomial coefficient $\binom{N}{m}$ ways to choose $m$ events from $N$ events. The expected value of the binomial distribution is given by:

$$
E[m] = \sum_{m=0}^{N} mP(m) = N\mu
$$

The variance of the binomial distribution is given by:

$$
Var[m] = E[m^2] - E[m]^2 = N\mu(1-\mu)
$$

### 2.1.1 The Beta Distribution

We have discussed above that if we only maximize the likelihood function, we may meet over-fitting problem. And the key to Bayesian theory is using prior probility to estimate the posterior probability. Previously, we only consider $\mu$ is a constant, we can regard $\mu$ as a random variable and use the prior probability $p(\mu)$ to estimate the posterior probability.

Conjudacy prior distribution: If the prior distribution $p(\mu)$ has the same distribution form(e.g. both Gauss Distribution) as the posterior distribution $p(\mu|\mathcal{D})$, the prior distribution is called the conjudacy prior distribution of the likelihood function.

Due to the prior distribution is random, we can choose any form of the prior distribution. We need to choose one that is easy to calculate for the posterior distribution. So we can choose the Beta distribution as the prior distribution of the Bernoulli distribution. The Beta distribution is given by:

$$
Beta(\mu|a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}
$$

where $a$ and $b$ are the parameters of the Beta distribution, $\Gamma(x)$ is the gamma function which is given by:

$$
\Gamma(x) = \int_{0}^{\infty} u^{x-1}e^{-u}du
$$

The expected value of the Beta distribution is given by:

$$
E[\mu] = \frac{a}{a+b}
$$

The variance of the Beta distribution is given by:

$$
Var[\mu] = \frac{ab}{(a+b)^2(a+b+1)}
$$

In beta distrbution, the $\gamma$ function is used to normalized the distribution, ensuring the integral of the distribution is 1. For any $a$ and $b$, the integral of the Beta distribution is 1, so that:

$$
\int_{0}^{1} Beta(\mu|a,b)d\mu = 1
$$

![beta distribution](/PRML/Chapter%202%20Probability%20Distributions/fig/image1.png)


The image above shows the curve in different hyperparameters. Let's back to the question. We need to choose the conjugate prior distribution of the Bernoulli distribution and we have already chosen the beta distribution. Consider how to calculate the posterior distribution of the Bernoulli distribution. The posterior distribution is given by:

$$
P(\mu|\mathcal{D}) = \frac{P(\mathcal{D}|\mu)P(\mu)}{P(\mathcal{D})}
$$

where $P(\mathcal{D})$ is the normalization factor. The posterior distribution is proportional to the product of the likelihood function and the prior distribution. And we know that $P(D)$ is a constant, so $P(\mu|\mathcal{D}) \propto P(\mathcal{D}|\mu)P(\mu)$. The likelihood function in Bernoulli distribution is given by:

$$
P(\mathcal{D}|\mu) = \prod_{n=1}^{N} \mu^{x_n}(1-\mu)^{1-x_n}
$$

Combine the likelihood function and the prior distribution, we can get the posterior distribution of the Bernoulli distribution:

$$
P(\mu|\mathcal{D}) \propto \mu^{\sum_{n=1}^{N} x_n + a - 1}(1-\mu)^{N - \sum_{n=1}^{N} x_n + b - 1}
$$

It is still a Beta distribution and only the parameters are changed. The posterior distribution is given by:

$$
P(\mu|\mathcal{D}) = Beta(\mu|\sum_{n=1}^{N} x_n + a, N - \sum_{n=1}^{N} x_n + b)
$$

The prior distribution is given by $Beta(\mu|a,b)$. As we can see that, after observing the data set, the parameters of the prior distribution are changed. The benefit of using the beta distribution is the prior distribution and posterior distribution remain the same form. And when we calculate the expectation and variance of the posterior distribution, we can avoid the complex integral calculation and use the formula of the expectation and variance of the beta distribution.

![difference before and after ovserving](/PRML/Chapter%202%20Probability%20Distributions/fig/image2.png)

The image above shows the difference before and after observing the dataset. We have a prior distritbution and after we observe the dataset(likelihood function), the posterior distritbution changed. In the figure, we can see that the likelihood function is increased as $\mu$ increases. As the result, the posterior distribution is also increased compared with prior distribution along with the same value of $\mu$. At the same time, the over-fitting problem is also avoid in some degree, the posterior distribution is moew smooth than the likelihood function.

Let's have a more general discussion. The Machine Learning is aimed to update the parameter $\theta$ when observing the dataset $\mathcal{D}$ in different batches which can be formulated as a joint distribution $p(\theta,\mathcal{D})$. We have:

$$\mathbb{E}_\theta[\theta] = \mathbb{E}_\mathcal{D}[\mathbb{E}[\theta|\mathcal{D}]]$$

Because:


$$
\mathbb{E}_\theta[\theta] = \int \theta p(\theta)d\theta
$$

$$ 
\mathbb{E}_\mathcal{D}[\mathbb{E}[\theta|\mathcal{D}]] = \int \left \{ \int \theta p(\theta|\mathcal{D})d\theta\right \} p(\mathcal{D}) d\mathcal{D}
$$

$$
= \int \theta \left \{ \int  p(\theta,\mathcal{D}) d\mathcal{D}\right \} d\theta
$$

$$
= \int \theta p(\theta)d\theta
$$

$$
= \mathbb{E}_\theta[\theta]
$$

So it means the posterior mean of $\theta$ is equal to the prior mean of $\theta$. So if we use MLE to estimate the parameter $\theta$, it is unbiased estimation.

## 2.2 Multinomial Variables

In chapter 2.1, we consider a binary value $x \in \{0,1\}$ and the probability of the head of a coin is given by $\mu$. Now we consider a multinomial value $x \in \{1,2,\ldots,K\}$ and the probability of the $k$-th value is given by $\mu_k$. The probability of the multinomial value $x$ is given by:

$$
P(x|\mu) = \prod_{k=1}^{K} \mu_k^{x_k}
$$

where $\sum_{k=1}^{K} \mu_k = 1$. The distribution is called the multinomial distribution. The expected value of the multinomial distribution is given by:

$$
E[x_k] = \sum_{x_k=1}^{K} x_kP(x_k) = \mu_k
$$

And consider a data set $\mathcal{D} = \{x_1,x_2,\ldots,x_N\}$ where $x_i$ is a K-dimensional vector. The likelihood function of the data set $\mathcal{D}$ is given by:

$$
P(\mathcal{D}|\mu) = \prod_{n=1}^{N} \prod_{k=1}^{K} \mu_k^{x_{nk}} = \prod_{k=1}^{K} \mu_k^{(\sum_{n=1}^{N} x_{nk})} = \prod_{k=1}^{K} \mu_k^{m_k}
$$

where $m_k = \sum_{n=1}^{N} x_{nk}$ is the number of the $k$-th value in the data set and it represents the number of the $k$-th value occurs in the data set. The log likelihood function is given by:

$$
\ln P(\mathcal{D}|\mu) = \sum_{k=1}^{K} m_k \ln \mu_k
$$

And we have a constraints that $\sum_{k=1}^{K} \mu_k = 1$. We can use the Lagrange multiplier to solve the optimization problem. The Lagrange function is given by:

$$
L(\mu,\lambda) = \sum_{k=1}^{K} m_k \ln \mu_k + \lambda(\sum_{k=1}^{K} \mu_k - 1)
$$

Take the derivative of the Lagrange function with respect to $\mu_k$ and set it to zero:

$$
\frac{\partial L}{\partial \mu_k} = \frac{m_k}{\mu_k} + \lambda = 0
$$

And we can solve the above equation to get the maximum likelihood estimator of $\mu_k$:

$$
\mu_k = -\frac{m_k}{\lambda}
$$

What about the joint distribution of the multinomial distribution? The joint distribution of the multinomial distribution is given by:

$$
P(m_1,m_2,\dots,m_K|\mu) = \frac{N!}{\prod_{k=1}^{K} m_k!} \prod_{k=1}^{K} \mu_k^{m_k}
$$

where $N = \sum_{k=1}^{K} m_k$ is the total number of the data set. The joint distribution is a multinomial coefficient times the likelihood function. The multinomial coefficient is used to normalize the distribution, ensuring the integral of the distribution is 1. The expected value of the multinomial distribution is given by:

$$
E[m_k] = N\mu_k
$$

### 2.2.1 The Dirichlet Distribution

Similar to the Beta distribution, we can use the Dirichlet distribution as the prior distribution of the multinomial distribution. The Dirichlet distribution is given by:

$$
Dir(\mu|\alpha) = \frac{\Gamma(\alpha_0)}{\prod_{k=1}^{K} \Gamma(\alpha_k)} \prod_{k=1}^{K} \mu_k^{\alpha_k - 1}
$$

where $\alpha = \{\alpha_1,\alpha_2,\ldots,\alpha_K\}$ is the parameter of the Dirichlet distribution and $\alpha_0 = \sum_{k=1}^{K} \alpha_k$. $\alpha_0$ is used to normalize the distribution, ensuring the integral of the distribution is 1.And the $\Gamma(x)$ is the gamma function which is given by:

$$
\Gamma(x) = \int_{0}^{\infty} u^{x-1}e^{-u}du
$$

![gamma function](/PRML/Chapter%202%20Probability%20Distributions/fig/image3.png)

The image above shows the constraints over three variables $\mu_1,\mu_2,\mu_3$. We need to ensure the sum of the three variables is 1. So the bound is a 2-dimensional plane.

Back to the question. Combine the likelihood function and the prior distribution that using Dirichlet distribution, we can get the posterior distribution of the multinomial distribution:

$$
P(\mu|\mathcal{D}) = Dir(\mu|\alpha + m) = \frac{\Gamma(\alpha_0 + N)}{\prod_{k=1}^{K} \Gamma(\alpha_k + m_k)} \prod_{k=1}^{K} \mu_k^{\alpha_k + m_k - 1}
$$

where $m = \{m_1,m_2,\ldots,m_K\}$ is the number of the $k$-th value in the data set.

![different alpha](/PRML/Chapter%202%20Probability%20Distributions/fig/image4.png)


The image above shows when $N=3$, the curve of Dirichlet distribution along with different $\alpha$. The first one $\alpha_k = 0.1$, the second one $\alpha_k = 1$, the third one $\alpha_k = 10$.

And in fact, the beta distribution is a special case of the Dirichlet distribution when $K=2$.


## 2.3 The Gaussian Distribution

If the variable is continuous, the most common distribution is the Gaussian distribution. The Gaussian distribution is given by:

$$
\mathcal{N}(x|\mu,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{1/2}} \exp \left \{ -\frac{1}{2\sigma^2}(x-\mu)^2 \right \}
$$

where $\mu$ is the mean of the distribution and $\sigma^2$ is the variance of the distribution. The expected value of the Gaussian distribution is given by:

$$
E[x] = \int x\mathcal{N}(x|\mu,\sigma^2)dx = \mu
$$

The variance of the Gaussian distribution is given by:

$$
Var[x] = E[x^2] - E[x]^2 = \sigma^2
$$

If the prior distribution of the Gaussian distribution is given by:

$$
p(\mu) = \mathcal{N}(\mu|\mu_0,\sigma_0^2)
$$

And the likelihood function of the Gaussian distribution is given by:

$$
p(x|\mu) = \mathcal{N}(x|\mu,\sigma^2) 
$$

The posterior distribution of the Gaussian distribution is given by:

$$
p(\mu|x) = \mathcal{N}(\mu|\mu_N,\sigma_N^2) = \mathcal{N}(\mu|\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}x,\frac{\sigma^2\sigma_0^2}{N\sigma_0^2+\sigma^2})
$$

If the variable is a D-dimensional vector, the Gaussian distribution is given by:

$$
\mathcal{N}(\mathbf{x}|\mathbf{\mu},\mathbf{\Sigma}) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\mathbf{\Sigma}|^{1/2}} \exp \left \{ -\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu}) \right \}
$$

where $\mathbf{\mu}$ is the mean of the distribution and $\mathbf{\Sigma}$ is the covariance matrix of the distribution. Let's review the covariance matrix. The covariance matrix is given by:

$$
\mathbf{\Sigma} = \begin{bmatrix} \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1D} \\ \sigma_{21} & \sigma_{22} & \cdots & \sigma_{2D} \\ \vdots & \vdots & \ddots & \vdots \\ \sigma_{D1} & \sigma_{D2} & \cdots & \sigma_{DD} \end{bmatrix}
$$

Where $\sigma_{ij}$ is the covariance between the $i$-th variable and the $j$-th variable. The diagonal elements of the covariance matrix are the variance of the variables. The off-diagonal elements of the covariance matrix are the covariance between the variables. The equation to calculate covariance is given by:

$$
\sigma_{ij} = E[(x_i-\mu_i)(x_j-\mu_j)]
$$

In chapter 1, we know the maximum entropy theorem. The Gaussian distribution is the maximum entropy distribution when the mean and the variance are given. The entropy of the Gaussian distribution is given by:

$$
H[x] = \frac{D}{2}(1+\ln(2\pi\sigma^2))
$$

Another situation that Gaussian distribution is the central limit theorem. The central limit theorem says that the sum of a large number of independent random variables is approximately Gaussian distributed. The central limit theorem is the foundation of the Gaussian distribution in the real world.

![Gaussian distribution](/PRML/Chapter%202%20Probability%20Distributions/fig/image5.png)

The image above shows the trend of the independent random variables. The sum of the independent random variables is approximately Gaussian distributed. The more the number of the random variables, the more the Gaussian distribution is close to the sum of the random variables.

The equation of central limit theorem is given by:

$$
\lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^{N} x_n \sim \mathcal{N}(\mu,\frac{\sigma^2}{N})
$$

If there are two random variables $x_1$ and $x_2$ and the sum of the two random variables is approximately Gaussian distributed. The sum of the two random variables is given by:

$$
x = x_1 + x_2
$$

The mean of the sum of the two random variables is given by:

$$
E[x] = E[x_1] + E[x_2] = \mu_1 + \mu_2
$$

The variance of the sum of the two random variables is given by:

$$
Var[x] = Var[x_1] + Var[x_2] = \sigma_1^2 + \sigma_2^2
$$

Before continue considering the geometrical form of Gaussian distribution, we need to review the linear algebra. 

- Eigenvalue and eigenvector: The eigenvector of a matrix is a vector that does not change its direction when multiplied by the matrix. The eigenvalue of a matrix is a scalar that represents how the eigenvector is stretched or compressed when multiplied by the matrix. The equation of the eigenvector and eigenvalue is given by:

$$
\mathbf{A}\mathbf{v} = \lambda\mathbf{v}
$$

where $\mathbf{A}$ is a matrix, $\mathbf{v}$ is the eigenvector, and $\lambda$ is the eigenvalue. The eigenvector and eigenvalue can be calculated by the equation above. The eigenvalue is represented by $\lambda$. We know that matrix multiply a vector is a linear transformation, the eigenvalue tells us how many times the eigenvector is stretched or compressed when multiplied by the matrix **compared with eigenvector**.

Back to the Gaussian distribution. Let's consider the quadratic form of the Gaussian distribution. The quadratic form of the Gaussian distribution is given by:

$$
\Delta^2 = (\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})
$$

The quantity $\Delta$ is called the Mahalanobis distance. The Mahalanobis distance is a measure of the distance between a point and a distribution. The Mahalanobis distance is a generalization of the Euclidean distance. The Euclidean distance is a special case of the Mahalanobis distance when the covariance matrix is the identity matrix.

We know that the covariance matrix can be taken to be symmetric and positive definite. The covariance matrix can be diagonalized by the eigenvector and eigenvalue. The covariance matrix can be written as:

$$
\mathbf{\Sigma} = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^T
$$

where $\mathbf{U}$ is the matrix of the eigenvector and $\mathbf{\Lambda}$ is the diagonal matrix of the eigenvalue. The eigenvector and eigenvalue can be calculated by the equation above.

Similarly, the inverse of the covariance matrix can be written as:

$$
\mathbf{\Sigma}^{-1} = \mathbf{U}\mathbf{\Lambda}^{-1}\mathbf{U}^T
$$

### 2.3.1 Conditional Gaussian Distribution

Multiple random variables joint distribution can be seen as Gaussian Distribution. The conditional distribution of the Gaussian distribution is also Gaussian distribution as well as the marginal distribution.


### 2.3.3 Bayesian theorem for Gaussian variables

