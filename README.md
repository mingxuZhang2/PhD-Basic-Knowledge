# PhD-Basic-Knowledge

To enhance my foundational knowledge for research, I plan to study the basics of AI and ML. My research interests include LLM, RL, and ML, so I have chosen some classic textbooks and well-known courses to study.

- [ ]  [PRML](https://github.com/gerdm/prml): I have found a GitHub repository that contains notes and example code for PRML. I plan to use this repository to assist my study of PRML. I plan to complete one or two chapters per week for this book, which has 14 chapters in total, so I will finish it in three months. I will write code and derive the theories myself, documenting my progress in Markdown. I will upload my study progress to my GitHub repository.
    - [ âˆš ]  Introduction [Note](https://github.com/mingxuZhang2/PhD-Basic-Knowledge/tree/main/PRML/Chapter%201%20Introduction)
    - [ ]  Probability Distributions
    - [ ]  Linear Models for Regression
    - [ ]  Linear Models for Classification
    - [ ]  Neural Networks
    - [ ]  Kernel Methods
    - [ ]  Sparse Kernel Machines
    - [ ]  Graphical Models
    - [ ]  Mixture Models and EM
    - [ ]  Approximate Inference
    - [ ]  Sampling Methods
    - [ ]  Continuous Latent Variables
    - [ ]  Sequential Data
    - [ ]  Combining Models

- [ ]  [CS224N(For NLP Technique)](https://web.stanford.edu/class/cs224n/index.html#schedule): CS224N-2024 Spring is taught by Stanford. The course covers NLP techniques, including LLM, RLHF, DPO, and more. The problem is that there are no available teaching videos right now; only the 2023 course materials are accessible. Therefore, I will start by learning from the slides. CS224N includes tutorials and assignments. There are four assignments, and I will complete these assignments and take notes. I will complete at least one tutorial per week and upload my progress to my GitHub repository.
    - [ ]  Word Vectors
    - [ ]  Word Vectors and Language Models
    - [ ]  Backpropagation and Neural Network Basics
    - [ ]  Dependency Parsing
    - [ ]  Recurrent Neural Networks
    - [ ]  Sequence to Sequence Models and Machine Translation
    - [ ]  LLM intro
    - [ ]  Transformers
    - [ ]  Pretraining
    - [ ]  Post-training (RLHF, SFT, DPO)
    - [ ]  Benchmarking and Evaluation
    - [ ]  Efficient Neural Network Training
    - [ ]  Speech Brain-Computer Interface
    - [ ]  Reasoning and Agents
    - [ ]  Life after DPO
    - [ ]  ConvNets, Tree Recursive Neural Networks and Constituency Parsing
    - [ ]  An Introduction to Responsible NLP

- [ ]  [EasyRL](https://datawhalechina.github.io/easy-rl/): Easy RL is a tutorial about Reinforcement Learning recommended by Prof. Yaodong Yang. This book includes 13 chapters. Unlike PRML and CS224N, Easy RL has fewer coding tasks but more theory. Therefore, I will focus on the theoretical proof of each algorithm. I will also complete at least one chapter per week.
    - [ ]  Introduction to RL
    - [ ]  MDP
    - [ ]  Table Methods
    - [ ]  Policy Gradient
    - [ ]  PPO
    - [ ]  DQN Basic
    - [ ]  DQN Tricks
    - [ ]  DQN for Continue Actions Space
    - [ ]  AC Algorithm
    - [ ]  Sparse Reward
    - [ ]  Imitation Learning
    - [ ]  DDPG
    - [ ]  Introduction to AlphaStar
    
- [ ]  Research: I will spend half of my time enhancing my basic knowledge and the other half on research. I will conduct the following research projects.
    - [ ]  Weak-to-Strong Generalization.
    - [ ]  ADRC for Lagrange Multiplier Updating.

- [ ]  Paper Reading: Meanwhile, I think I need to read some classic papers, but I am not sure which ones to read yet. So this section remains to be discussed. I will read some LLM papers on weak-to-strong generalization and scalable oversight, which are related to my research. I will read at least three papers per week.
    - [ ]  [Weak-to-Strong Extrapolation Expedites Alignment](https://arxiv.org/abs/2404.16792)